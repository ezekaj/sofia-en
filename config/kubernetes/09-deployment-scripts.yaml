# Deployment Job for Initial Setup
apiVersion: batch/v1
kind: Job
metadata:
  name: sofia-deployment-init
  namespace: sofia-dental
  labels:
    app: sofia-dental
    component: deployment
spec:
  template:
    metadata:
      labels:
        app: sofia-deployment
    spec:
      restartPolicy: OnFailure
      initContainers:
      - name: wait-for-cluster
        image: bitnami/kubectl:latest
        command: ['sh', '-c']
        args:
        - |
          echo "Waiting for cluster readiness...";
          kubectl get nodes;
          kubectl get ns sofia-dental;
          echo "Cluster is ready for deployment";
      containers:
      - name: deployment-validator
        image: bitnami/kubectl:latest
        command: ['sh', '-c']
        args:
        - |
          echo "=== Sofia Voice Agent Deployment Validation ===";
          
          # Check namespace
          if kubectl get ns sofia-dental; then
            echo "✓ Namespace sofia-dental exists";
          else
            echo "✗ Namespace sofia-dental missing";
            exit 1;
          fi
          
          # Check secrets
          if kubectl get secret sofia-secrets -n sofia-dental; then
            echo "✓ Sofia secrets configured";
          else
            echo "✗ Sofia secrets missing";
            exit 1;
          fi
          
          # Check persistent volumes
          if kubectl get pvc -n sofia-dental | grep -q Bound; then
            echo "✓ Persistent volumes bound";
          else
            echo "⚠ Some persistent volumes not bound yet";
          fi
          
          # Wait for deployments to be ready
          echo "Waiting for deployments...";
          kubectl wait --for=condition=available --timeout=600s deployment/livekit-server -n sofia-dental || echo "⚠ LiveKit not ready yet";
          kubectl wait --for=condition=available --timeout=600s deployment/dental-calendar -n sofia-dental || echo "⚠ Calendar not ready yet";
          kubectl wait --for=condition=available --timeout=600s deployment/sofia-agent -n sofia-dental || echo "⚠ Sofia Agent not ready yet";
          
          echo "=== Deployment Status ===";
          kubectl get all -n sofia-dental;
          
          echo "=== Service Endpoints ===";
          kubectl get svc -n sofia-dental -o wide;
          
          echo "=== Ingress Status ===";
          kubectl get ingress -n sofia-dental;
          
          echo "Deployment validation complete!";
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
      serviceAccountName: deployment-sa
---
# Service Account for Deployment Jobs
apiVersion: v1
kind: ServiceAccount
metadata:
  name: deployment-sa
  namespace: sofia-dental
  labels:
    app: sofia-dental
    component: deployment
---
# Role for Deployment Operations
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: deployment-role
  namespace: sofia-dental
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints", "persistentvolumeclaims", "configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch"]
---
# Role Binding for Deployment
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: deployment-rolebinding
  namespace: sofia-dental
subjects:
- kind: ServiceAccount
  name: deployment-sa
  namespace: sofia-dental
roleRef:
  kind: Role
  name: deployment-role
  apiGroup: rbac.authorization.k8s.io
---
# Health Check CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sofia-health-check
  namespace: sofia-dental
  labels:
    app: sofia-dental
    component: monitoring
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: health-checker
        spec:
          restartPolicy: OnFailure
          containers:
          - name: health-checker
            image: curlimages/curl:latest
            command: ['sh', '-c']
            args:
            - |
              echo "=== Sofia Health Check $(date) ===";
              
              # Check Sofia Web Interface
              if curl -f -s http://sofia-web-service:5001/health > /dev/null; then
                echo "✓ Sofia Web: Healthy";
              else
                echo "✗ Sofia Web: Unhealthy";
                UNHEALTHY=1;
              fi
              
              # Check Sofia Agent
              if curl -f -s http://sofia-agent-service:8080/health > /dev/null; then
                echo "✓ Sofia Agent: Healthy";
              else
                echo "✗ Sofia Agent: Unhealthy";
                UNHEALTHY=1;
              fi
              
              # Check LiveKit
              if curl -f -s http://livekit-service:7880/ > /dev/null; then
                echo "✓ LiveKit: Healthy";
              else
                echo "✗ LiveKit: Unhealthy";
                UNHEALTHY=1;
              fi
              
              # Check Calendar Service
              if curl -f -s http://dental-calendar-service:3005/health > /dev/null; then
                echo "✓ Calendar: Healthy";
              else
                echo "✗ Calendar: Unhealthy";
                UNHEALTHY=1;
              fi
              
              # Check Prometheus
              if curl -f -s http://prometheus-service:9090/-/healthy > /dev/null; then
                echo "✓ Prometheus: Healthy";
              else
                echo "✗ Prometheus: Unhealthy";
                UNHEALTHY=1;
              fi
              
              if [ "$UNHEALTHY" = "1" ]; then
                echo "⚠ Some services are unhealthy";
                exit 1;
              else
                echo "✓ All services healthy";
              fi
            resources:
              requests:
                memory: "32Mi"
                cpu: "50m"
              limits:
                memory: "64Mi"
                cpu: "100m"
          serviceAccountName: health-check-sa
---
# Service Account for Health Checks
apiVersion: v1
kind: ServiceAccount
metadata:
  name: health-check-sa
  namespace: sofia-dental
---
# Backup CronJob for Database
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sofia-backup
  namespace: sofia-dental
  labels:
    app: sofia-dental
    component: backup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: backup
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup
            image: sqlite3:latest
            command: ['sh', '-c']
            args:
            - |
              DATE=$(date +%Y%m%d_%H%M%S);
              echo "Starting backup at $DATE";
              
              # Backup calendar database
              if [ -f /app/data/dental_calendar.db ]; then
                cp /app/data/dental_calendar.db /backups/dental_calendar_$DATE.db;
                echo "✓ Calendar database backed up";
              else
                echo "⚠ Calendar database not found";
              fi
              
              # Backup patient data
              if [ -f /app/data/patients.json ]; then
                cp /app/data/patients.json /backups/patients_$DATE.json;
                echo "✓ Patient data backed up";
              else
                echo "⚠ Patient data not found";
              fi
              
              # Cleanup old backups (keep 30 days)
              find /backups -name "*.db" -mtime +30 -delete;
              find /backups -name "*.json" -mtime +30 -delete;
              
              echo "Backup completed at $(date)";
            volumeMounts:
            - name: calendar-data
              mountPath: /app/data
            - name: backup-storage
              mountPath: /backups
            resources:
              requests:
                memory: "64Mi"
                cpu: "100m"
              limits:
                memory: "128Mi"
                cpu: "200m"
          volumes:
          - name: calendar-data
            persistentVolumeClaim:
              claimName: calendar-data-pvc
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
---
# Backup Storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: sofia-dental
  labels:
    app: sofia-dental
    component: backup
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Auto-restart Job for Failed Pods
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sofia-auto-restart
  namespace: sofia-dental
  labels:
    app: sofia-dental
    component: maintenance
spec:
  schedule: "*/10 * * * *"  # Every 10 minutes
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: auto-restart
        spec:
          restartPolicy: OnFailure
          containers:
          - name: auto-restart
            image: bitnami/kubectl:latest
            command: ['sh', '-c']
            args:
            - |
              echo "Checking for failed pods...";
              
              # Get pods in Error, CrashLoopBackOff, or ImagePullBackOff state
              FAILED_PODS=$(kubectl get pods -n sofia-dental --field-selector=status.phase=Failed -o name);
              CRASHLOOP_PODS=$(kubectl get pods -n sofia-dental -o json | jq -r '.items[] | select(.status.containerStatuses[]?.state.waiting?.reason=="CrashLoopBackOff") | .metadata.name');
              
              if [ ! -z "$FAILED_PODS" ]; then
                echo "Deleting failed pods: $FAILED_PODS";
                echo $FAILED_PODS | xargs kubectl delete -n sofia-dental;
              fi
              
              if [ ! -z "$CRASHLOOP_PODS" ]; then
                echo "Restarting crashloop pods: $CRASHLOOP_PODS";
                for pod in $CRASHLOOP_PODS; do
                  kubectl delete pod $pod -n sofia-dental;
                done
              fi
              
              # Check deployment health and restart if needed
              for deployment in sofia-agent livekit-server dental-calendar sofia-web; do
                READY=$(kubectl get deployment $deployment -n sofia-dental -o jsonpath='{.status.readyReplicas}');
                DESIRED=$(kubectl get deployment $deployment -n sofia-dental -o jsonpath='{.spec.replicas}');
                
                if [ "$READY" != "$DESIRED" ]; then
                  echo "Deployment $deployment is not healthy ($READY/$DESIRED ready)";
                  kubectl rollout restart deployment/$deployment -n sofia-dental;
                fi
              done
              
              echo "Auto-restart check completed";
            resources:
              requests:
                memory: "64Mi"
                cpu: "100m"
              limits:
                memory: "128Mi"
                cpu: "200m"
          serviceAccountName: auto-restart-sa
---
# Service Account for Auto-restart
apiVersion: v1
kind: ServiceAccount
metadata:
  name: auto-restart-sa
  namespace: sofia-dental
---
# Role for Auto-restart Operations
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: auto-restart-role
  namespace: sofia-dental
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "patch"]
---
# Role Binding for Auto-restart
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: auto-restart-rolebinding
  namespace: sofia-dental
subjects:
- kind: ServiceAccount
  name: auto-restart-sa
  namespace: sofia-dental
roleRef:
  kind: Role
  name: auto-restart-role
  apiGroup: rbac.authorization.k8s.io
